# Keywording Benchmark Competition
This repo documents the keywording computer vision benchmark we did.

MTurk evaluation results:

```
Name             Labels/Image   Precision   Unique Concepts
AWS Rekognition         13.80        0.81              1026
Clarifai                20.00        0.84              1467
Google Cloud Vision      8.70        0.92              1140
Mobius Labs             16.30        0.92              1018
```

Expert evaluation results:

```
Name              Acceptance Ratio
AWS Rekognition               0.34
Clarifai                      0.40
Google Cloud Vision           0.47
Mobius Labs                   0.80
```

Check out [compute evaluation results.ipynb](https://github.com/mobiusml/benchmark_competition/blob/master/compute%20evaluation%20results.ipynb) and [compute expert evaluation results.ipynb](https://github.com/mobiusml/benchmark_competition/blob/master/compute%20expert%20evaluation%20results.ipynb) to find out how to access the data and how we calculated the numbers.
